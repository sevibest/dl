{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefe61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ameyp\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Vocabulary Size: 13\n",
      "WARNING:tensorflow:From C:\\Users\\ameyp\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\ameyp\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\ameyp\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1/1 [==============================] - 1s 994ms/step - loss: 2.5725\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5672\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.5619\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.5567\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5515\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.5462\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5410\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.5358\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5305\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5253\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5201\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5148\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.5095\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.5043\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4989\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4936\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4882\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.4828\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4774\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4719\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4664\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4608\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.4552\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.4495\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.4437\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.4380\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4321\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4262\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.4202\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.4141\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.4080\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.4018\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3955\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3891\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3827\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3762\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.3696\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.3629\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.3561\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3493\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3423\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.3353\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.3282\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.3210\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.3137\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.3064\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.2989\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2914\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.2837\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2760\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2682\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2603\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.2524\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.2443\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2362\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.2279\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2196\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.2112\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.2027\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1942\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.1856\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1768\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.1681\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.1592\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.1502\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1412\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.1321\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.1230\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.1137\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.1044\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 2.0950\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 2.0856\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 2.0761\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 2.0665\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0568\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0471\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 2.0374\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 2.0275\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 2.0176\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 2.0077\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.9977\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9876\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.9775\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.9673\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.9570\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9467\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9364\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.9260\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.9156\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 1.9051\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8945\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8840\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8733\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step - loss: 1.8627\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.8519\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 1.8412\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8304\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8195\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.8087\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 1.7978\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "Context: ['quick', 'brown', 'jumped', 'over'] -> Predicted Target Word: 'the'\n"
     ]
    }
   ],
   "source": [
    "# a. Data preparation\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Lambda\n",
    "import tensorflow.keras.backend as K\n",
    "import random\n",
    "\n",
    "# Example corpus (small for simplicity, use a larger one for better results)\n",
    "corpus = [\n",
    "    \"the quick brown fox jumped over the lazy dog\",\n",
    "    \"the dog barked at the fox\",\n",
    "    \"the fox is quick and the dog is lazy\",\n",
    "    \"the quick dog jumped over the lazy fox\"\n",
    "]\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v: k for k, v in word2idx.items()}\n",
    "vocab_size = len(word2idx) + 1  # Add 1 for padding (if needed)\n",
    "print(f'Vocabulary Size: {vocab_size}')\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "# b. Generate training data\n",
    "# Context window size: 2 words before and after the target word\n",
    "window_size = 2\n",
    "\n",
    "def generate_context_target_pairs(sequences, window_size):\n",
    "    context_target_pairs = []\n",
    "    for sequence in sequences:\n",
    "        for i, word in enumerate(sequence):\n",
    "            # Define the context window range\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(sequence), i + window_size + 1)\n",
    "            context = [sequence[j] for j in range(start, end) if j != i]\n",
    "            target = word\n",
    "            if len(context) == window_size * 2:\n",
    "                context_target_pairs.append((context, target))\n",
    "    return context_target_pairs\n",
    "\n",
    "# Generate the training data (context and target pairs)\n",
    "pairs = generate_context_target_pairs(sequences, window_size)\n",
    "\n",
    "# Split context and target words\n",
    "contexts, targets = zip(*pairs)\n",
    "contexts = np.array(contexts)\n",
    "targets = np.array(targets)\n",
    "\n",
    "# One-hot encode the target words\n",
    "targets = to_categorical(targets, num_classes=vocab_size)\n",
    "\n",
    "# c. Train the model\n",
    "\n",
    "# Define CBOW Model\n",
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=window_size * 2))\n",
    "model.add(Lambda(lambda x: K.mean(x, axis=1)))  # Average embeddings for context words\n",
    "model.add(Dense(vocab_size, activation='softmax'))  # Output softmax over vocabulary\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train the model\n",
    "model.fit(contexts, targets, epochs=100, verbose=1)\n",
    "\n",
    "# d. Output (Predict target word from context)\n",
    "# Predict the target word for a given context\n",
    "\n",
    "def predict_target(context_words):\n",
    "    # Convert words to their respective indices\n",
    "    context_indices = [word2idx[word] for word in context_words]\n",
    "    context_indices = np.array(context_indices).reshape(1, window_size * 2)\n",
    "    \n",
    "    # Predict the target word's index\n",
    "    predicted_idx = np.argmax(model.predict(context_indices), axis=1)[0]\n",
    "    \n",
    "    # Convert index back to word\n",
    "    return idx2word[predicted_idx]\n",
    "\n",
    "# Example: Predict the target word from the context ['quick', 'brown', 'jumped', 'over']\n",
    "context_words = ['quick', 'brown', 'jumped', 'over']\n",
    "predicted_word = predict_target(context_words)\n",
    "print(f\"Context: {context_words} -> Predicted Target Word: '{predicted_word}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b969a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
